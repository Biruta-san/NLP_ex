{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d92cad",
   "metadata": {},
   "source": [
    "# Mecanismo de Busca Semântica para Perguntas Médicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0038fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar dependencias\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7535d24",
   "metadata": {},
   "source": [
    "## Passo 1: Configuração e Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ab9a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando o conjunto de dados MedQuad do KaggleHub...\n",
      "Dataset baixado com sucesso em: /home/codespace/.cache/kagglehub/datasets/pythonafroz/medquad-medical-question-answer-for-ai-research/versions/1\n",
      "Carregando o arquivo CSV para um DataFrame pandas...\n",
      "Dataset carregado. Número de pares de pergunta-resposta: 16407\n",
      "Utilizando o dispositivo: cpu\n",
      "Carregando o modelo e o tokenizador da Hugging Face...\n"
     ]
    }
   ],
   "source": [
    "print(\"Baixando o conjunto de dados MedQuad do KaggleHub...\")\n",
    "# Faz o download da versão mais recente do dataset\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"pythonafroz/medquad-medical-question-answer-for-ai-research\")\n",
    "    print(f\"Dataset baixado com sucesso em: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao baixar o dataset. Verifique sua autenticação do Kaggle. Erro: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Carregando o arquivo CSV para um DataFrame pandas...\")\n",
    "# Carrega o dataset\n",
    "try:\n",
    "    df = pd.read_csv(f\"{path}/medquad.csv\")\n",
    "    # Limpeza básica: remove linhas onde a pergunta ou a resposta estão vazias\n",
    "    df.dropna(subset=['question', 'answer'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Dataset carregado. Número de pares de pergunta-resposta: {len(df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Arquivo medquad.csv não encontrado no caminho: {path}\")\n",
    "    exit()\n",
    "\n",
    "# Define o dispositivo (GPU se disponível, caso contrário CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilizando o dispositivo: {device}\")\n",
    "\n",
    "print(\"Carregando o modelo e o tokenizador da Hugging Face...\")\n",
    "# Modelo pré-treinado para a língua inglesa\n",
    "nome_modelo = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(nome_modelo)\n",
    "model = AutoModel.from_pretrained(nome_modelo).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8af7ce",
   "metadata": {},
   "source": [
    "## Passo 2: Função para Geração de Embeddings (Mean Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574cfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Gera embeddings para uma lista de textos usando o modelo BERT.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): Uma lista de sentenças/textos.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Um tensor contendo os embeddings dos textos.\n",
    "    \"\"\"\n",
    "    # Tokeniza as sentenças e move os tensores para o dispositivo (GPU/CPU)\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Desativa o cálculo de gradientes para economizar memória e acelerar\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Mean Pooling: Calcula a média dos embeddings dos tokens da última camada\n",
    "    # para obter um único vetor que representa a sentença inteira.\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    mean_pooled = sum_embeddings / sum_mask\n",
    "    \n",
    "    # Normaliza os embeddings para otimizar o cálculo da similaridade de cossenos\n",
    "    return F.normalize(mean_pooled, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864956d7",
   "metadata": {},
   "source": [
    "## Passo 3: Indexação - Gerando Embeddings para Todas as Perguntas do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df36467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a indexação: gerando embeddings para todas as perguntas do dataset...\n",
      "Este processo pode demorar alguns minutos dependendo do hardware...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "print(\"Iniciando a indexação: gerando embeddings para todas as perguntas do dataset...\")\n",
    "print(\"Este processo pode demorar alguns minutos dependendo do hardware...\")\n",
    "\n",
    "# Converte a coluna de perguntas para uma lista\n",
    "questions_list = df['question'].tolist()\n",
    "\n",
    "# Gera os embeddings para todas as perguntas\n",
    "# Para datasets muito grandes, isso deve ser feito em lotes (batches)\n",
    "# Aqui, para simplicidade, passamos a lista inteira\n",
    "dataset_embeddings = generate_embeddings(questions_list).cpu() # Move para a CPU para a busca\n",
    "\n",
    "print(\"Indexação concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d1e59",
   "metadata": {},
   "source": [
    "## Passo 4: Função de Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc669153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_answer(user_question, dataset_embeddings, data_frame):\n",
    "    # Gera o embedding para a pergunta do usuário\n",
    "    query_embedding = generate_embeddings([user_question]).cpu()\n",
    "\n",
    "    # Calcula a similaridade de cossenos entre a pergunta do usuário e todas as perguntas do dataset\n",
    "    # A similaridade de cossenos varia de -1 (opostos) a 1 (idênticos)\n",
    "    cosine_scores = torch.matmul(query_embedding, dataset_embeddings.T)\n",
    "\n",
    "    # Encontra o índice da pergunta com a maior similaridade\n",
    "    best_match_index = torch.argmax(cosine_scores).item()\n",
    "\n",
    "    # Recupera a pergunta, a resposta e o score do DataFrame\n",
    "    best_question = data_frame.iloc[best_match_index]['question']\n",
    "    best_answer = data_frame.iloc[best_match_index]['answer']\n",
    "    similarity_score = cosine_scores[0][best_match_index].item()\n",
    "\n",
    "    return best_question, best_answer, similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5428bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Mecanismo de Busca Semântica Médica ---\")\n",
    "print(\"Digite sua pergunta em inglês. Digite 'sair' para terminar.\")\n",
    "\n",
    "while True:\n",
    "    # Permite que o usuário insira uma pergunta\n",
    "    user_query = input(\"\\nDigite 'sair' para encerrar a aplicação\\nSua pergunta: \")\n",
    "    \n",
    "    if user_query.lower() == 'sair':\n",
    "        break\n",
    "\n",
    "    # Busca a resposta\n",
    "    similar_q, answer, score = find_best_answer(user_query, dataset_embeddings, df)\n",
    "\n",
    "    # Exibe os resultados\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Pergunta do Usuário: {user_query}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Pergunta Mais Similar Encontrada (Score: {score:.4f}): {similar_q}\")\n",
    "    print(\"\\nResposta Encontrada:\")\n",
    "    print(answer)\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
